-- CHANGE WITH CAUTION: This is a generated code file generated by https://github.com/Haskell-OpenAPI-Code-Generator/Haskell-OpenAPI-Client-Code-Generator.

{-# LANGUAGE OverloadedStrings #-}
{-# LANGUAGE MultiWayIf #-}

-- | Contains the types generated from the schema CreateChatCompletionResponse
module OpenAI.Types.CreateChatCompletionResponse where

import qualified Prelude as GHC.Integer.Type
import qualified Prelude as GHC.Maybe
import qualified Control.Monad.Fail
import qualified Data.Aeson
import qualified Data.Aeson as Data.Aeson.Encoding.Internal
import qualified Data.Aeson as Data.Aeson.Types
import qualified Data.Aeson as Data.Aeson.Types.FromJSON
import qualified Data.Aeson as Data.Aeson.Types.ToJSON
import qualified Data.Aeson as Data.Aeson.Types.Internal
import qualified Data.ByteString
import qualified Data.ByteString as Data.ByteString.Internal
import qualified Data.Foldable
import qualified Data.Functor
import qualified Data.Maybe
import qualified Data.Scientific
import qualified Data.Text
import qualified Data.Text as Data.Text.Internal
import qualified Data.Time.Calendar as Data.Time.Calendar.Days
import qualified Data.Time.LocalTime as Data.Time.LocalTime.Internal.ZonedTime
import qualified GHC.Base
import qualified GHC.Classes
import qualified GHC.Int
import qualified GHC.Show
import qualified GHC.Types
import qualified OpenAI.Common
import OpenAI.TypeAlias
import {-# SOURCE #-} OpenAI.Types.ChatCompletionResponseMessage
import {-# SOURCE #-} OpenAI.Types.ChatCompletionTokenLogprob
import {-# SOURCE #-} OpenAI.Types.CompletionUsage

-- | Defines the object schema located at @components.schemas.CreateChatCompletionResponse@ in the specification.
-- 
-- Represents a chat completion response returned by model, based on the provided input.
data CreateChatCompletionResponse = CreateChatCompletionResponse {
  -- | choices: A list of chat completion choices. Can be more than one if \`n\` is greater than 1.
  createChatCompletionResponseChoices :: ([CreateChatCompletionResponseChoices'])
  -- | created: The Unix timestamp (in seconds) of when the chat completion was created.
  , createChatCompletionResponseCreated :: GHC.Types.Int
  -- | id: A unique identifier for the chat completion.
  , createChatCompletionResponseId :: Data.Text.Internal.Text
  -- | model: The model used for the chat completion.
  , createChatCompletionResponseModel :: Data.Text.Internal.Text
  -- | system_fingerprint: This fingerprint represents the backend configuration that the model runs with.
  -- 
  -- Can be used in conjunction with the \`seed\` request parameter to understand when backend changes have been made that might impact determinism.
  , createChatCompletionResponseSystemFingerprint :: (GHC.Maybe.Maybe Data.Text.Internal.Text)
  -- | usage: Usage statistics for the completion request.
  , createChatCompletionResponseUsage :: (GHC.Maybe.Maybe CompletionUsage)
  } deriving (GHC.Show.Show
  , GHC.Classes.Eq)
instance Data.Aeson.Types.ToJSON.ToJSON CreateChatCompletionResponse
    where {toJSON obj = Data.Aeson.Types.Internal.object (Data.Foldable.concat (["choices" Data.Aeson.Types.ToJSON..= createChatCompletionResponseChoices obj] : ["created" Data.Aeson.Types.ToJSON..= createChatCompletionResponseCreated obj] : ["id" Data.Aeson.Types.ToJSON..= createChatCompletionResponseId obj] : ["model" Data.Aeson.Types.ToJSON..= createChatCompletionResponseModel obj] : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("system_fingerprint" Data.Aeson.Types.ToJSON..=)) (createChatCompletionResponseSystemFingerprint obj) : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("usage" Data.Aeson.Types.ToJSON..=)) (createChatCompletionResponseUsage obj) : ["object" Data.Aeson.Types.ToJSON..= Data.Aeson.Types.Internal.String "chat.completion"] : GHC.Base.mempty));
           toEncoding obj = Data.Aeson.Encoding.Internal.pairs (GHC.Base.mconcat (Data.Foldable.concat (["choices" Data.Aeson.Types.ToJSON..= createChatCompletionResponseChoices obj] : ["created" Data.Aeson.Types.ToJSON..= createChatCompletionResponseCreated obj] : ["id" Data.Aeson.Types.ToJSON..= createChatCompletionResponseId obj] : ["model" Data.Aeson.Types.ToJSON..= createChatCompletionResponseModel obj] : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("system_fingerprint" Data.Aeson.Types.ToJSON..=)) (createChatCompletionResponseSystemFingerprint obj) : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("usage" Data.Aeson.Types.ToJSON..=)) (createChatCompletionResponseUsage obj) : ["object" Data.Aeson.Types.ToJSON..= Data.Aeson.Types.Internal.String "chat.completion"] : GHC.Base.mempty)))}
instance Data.Aeson.Types.FromJSON.FromJSON CreateChatCompletionResponse
    where {parseJSON = Data.Aeson.Types.FromJSON.withObject "CreateChatCompletionResponse" (\obj -> (((((GHC.Base.pure CreateChatCompletionResponse GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..: "choices")) GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..: "created")) GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..: "id")) GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..: "model")) GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..:! "system_fingerprint")) GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..:! "usage"))}
-- | Create a new 'CreateChatCompletionResponse' with all required fields.
mkCreateChatCompletionResponse :: [CreateChatCompletionResponseChoices'] -- ^ 'createChatCompletionResponseChoices'
  -> GHC.Types.Int -- ^ 'createChatCompletionResponseCreated'
  -> Data.Text.Internal.Text -- ^ 'createChatCompletionResponseId'
  -> Data.Text.Internal.Text -- ^ 'createChatCompletionResponseModel'
  -> CreateChatCompletionResponse
mkCreateChatCompletionResponse createChatCompletionResponseChoices createChatCompletionResponseCreated createChatCompletionResponseId createChatCompletionResponseModel = CreateChatCompletionResponse{createChatCompletionResponseChoices = createChatCompletionResponseChoices,
                                                                                                                                                                                                       createChatCompletionResponseCreated = createChatCompletionResponseCreated,
                                                                                                                                                                                                       createChatCompletionResponseId = createChatCompletionResponseId,
                                                                                                                                                                                                       createChatCompletionResponseModel = createChatCompletionResponseModel,
                                                                                                                                                                                                       createChatCompletionResponseSystemFingerprint = GHC.Maybe.Nothing,
                                                                                                                                                                                                       createChatCompletionResponseUsage = GHC.Maybe.Nothing}
-- | Defines the object schema located at @components.schemas.CreateChatCompletionResponse.properties.choices.items@ in the specification.
-- 
-- 
data CreateChatCompletionResponseChoices' = CreateChatCompletionResponseChoices' {
  -- | finish_reason: The reason the model stopped generating tokens. This will be \`stop\` if the model hit a natural stop point or a provided stop sequence,
  -- \`length\` if the maximum number of tokens specified in the request was reached,
  -- \`content_filter\` if content was omitted due to a flag from our content filters,
  -- \`tool_calls\` if the model called a tool, or \`function_call\` (deprecated) if the model called a function.
  createChatCompletionResponseChoices'FinishReason :: CreateChatCompletionResponseChoices'FinishReason'
  -- | index: The index of the choice in the list of choices.
  , createChatCompletionResponseChoices'Index :: GHC.Types.Int
  -- | logprobs: Log probability information for the choice.
  , createChatCompletionResponseChoices'Logprobs :: (OpenAI.Common.Nullable CreateChatCompletionResponseChoices'Logprobs'NonNullable)
  -- | message: A chat completion message generated by the model.
  , createChatCompletionResponseChoices'Message :: ChatCompletionResponseMessage
  } deriving (GHC.Show.Show
  , GHC.Classes.Eq)
instance Data.Aeson.Types.ToJSON.ToJSON CreateChatCompletionResponseChoices'
    where {toJSON obj = Data.Aeson.Types.Internal.object (Data.Foldable.concat (["finish_reason" Data.Aeson.Types.ToJSON..= createChatCompletionResponseChoices'FinishReason obj] : ["index" Data.Aeson.Types.ToJSON..= createChatCompletionResponseChoices'Index obj] : ["logprobs" Data.Aeson.Types.ToJSON..= createChatCompletionResponseChoices'Logprobs obj] : ["message" Data.Aeson.Types.ToJSON..= createChatCompletionResponseChoices'Message obj] : GHC.Base.mempty));
           toEncoding obj = Data.Aeson.Encoding.Internal.pairs (GHC.Base.mconcat (Data.Foldable.concat (["finish_reason" Data.Aeson.Types.ToJSON..= createChatCompletionResponseChoices'FinishReason obj] : ["index" Data.Aeson.Types.ToJSON..= createChatCompletionResponseChoices'Index obj] : ["logprobs" Data.Aeson.Types.ToJSON..= createChatCompletionResponseChoices'Logprobs obj] : ["message" Data.Aeson.Types.ToJSON..= createChatCompletionResponseChoices'Message obj] : GHC.Base.mempty)))}
instance Data.Aeson.Types.FromJSON.FromJSON CreateChatCompletionResponseChoices'
    where {parseJSON = Data.Aeson.Types.FromJSON.withObject "CreateChatCompletionResponseChoices'" (\obj -> (((GHC.Base.pure CreateChatCompletionResponseChoices' GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..: "finish_reason")) GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..: "index")) GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..: "logprobs")) GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..: "message"))}
-- | Create a new 'CreateChatCompletionResponseChoices'' with all required fields.
mkCreateChatCompletionResponseChoices' :: CreateChatCompletionResponseChoices'FinishReason' -- ^ 'createChatCompletionResponseChoices'FinishReason'
  -> GHC.Types.Int -- ^ 'createChatCompletionResponseChoices'Index'
  -> OpenAI.Common.Nullable CreateChatCompletionResponseChoices'Logprobs'NonNullable -- ^ 'createChatCompletionResponseChoices'Logprobs'
  -> ChatCompletionResponseMessage -- ^ 'createChatCompletionResponseChoices'Message'
  -> CreateChatCompletionResponseChoices'
mkCreateChatCompletionResponseChoices' createChatCompletionResponseChoices'FinishReason createChatCompletionResponseChoices'Index createChatCompletionResponseChoices'Logprobs createChatCompletionResponseChoices'Message = CreateChatCompletionResponseChoices'{createChatCompletionResponseChoices'FinishReason = createChatCompletionResponseChoices'FinishReason,
                                                                                                                                                                                                                                                                  createChatCompletionResponseChoices'Index = createChatCompletionResponseChoices'Index,
                                                                                                                                                                                                                                                                  createChatCompletionResponseChoices'Logprobs = createChatCompletionResponseChoices'Logprobs,
                                                                                                                                                                                                                                                                  createChatCompletionResponseChoices'Message = createChatCompletionResponseChoices'Message}
-- | Defines the enum schema located at @components.schemas.CreateChatCompletionResponse.properties.choices.items.properties.finish_reason@ in the specification.
-- 
-- The reason the model stopped generating tokens. This will be \`stop\` if the model hit a natural stop point or a provided stop sequence,
-- \`length\` if the maximum number of tokens specified in the request was reached,
-- \`content_filter\` if content was omitted due to a flag from our content filters,
-- \`tool_calls\` if the model called a tool, or \`function_call\` (deprecated) if the model called a function.
data CreateChatCompletionResponseChoices'FinishReason' =
   CreateChatCompletionResponseChoices'FinishReason'Other Data.Aeson.Types.Internal.Value -- ^ This case is used if the value encountered during decoding does not match any of the provided cases in the specification.
  | CreateChatCompletionResponseChoices'FinishReason'Typed Data.Text.Internal.Text -- ^ This constructor can be used to send values to the server which are not present in the specification yet.
  | CreateChatCompletionResponseChoices'FinishReason'EnumStop -- ^ Represents the JSON value @"stop"@
  | CreateChatCompletionResponseChoices'FinishReason'EnumLength -- ^ Represents the JSON value @"length"@
  | CreateChatCompletionResponseChoices'FinishReason'EnumToolCalls -- ^ Represents the JSON value @"tool_calls"@
  | CreateChatCompletionResponseChoices'FinishReason'EnumContentFilter -- ^ Represents the JSON value @"content_filter"@
  | CreateChatCompletionResponseChoices'FinishReason'EnumFunctionCall -- ^ Represents the JSON value @"function_call"@
  deriving (GHC.Show.Show, GHC.Classes.Eq)
instance Data.Aeson.Types.ToJSON.ToJSON CreateChatCompletionResponseChoices'FinishReason'
    where {toJSON (CreateChatCompletionResponseChoices'FinishReason'Other val) = val;
           toJSON (CreateChatCompletionResponseChoices'FinishReason'Typed val) = Data.Aeson.Types.ToJSON.toJSON val;
           toJSON (CreateChatCompletionResponseChoices'FinishReason'EnumStop) = "stop";
           toJSON (CreateChatCompletionResponseChoices'FinishReason'EnumLength) = "length";
           toJSON (CreateChatCompletionResponseChoices'FinishReason'EnumToolCalls) = "tool_calls";
           toJSON (CreateChatCompletionResponseChoices'FinishReason'EnumContentFilter) = "content_filter";
           toJSON (CreateChatCompletionResponseChoices'FinishReason'EnumFunctionCall) = "function_call"}
instance Data.Aeson.Types.FromJSON.FromJSON CreateChatCompletionResponseChoices'FinishReason'
    where {parseJSON val = GHC.Base.pure (if | val GHC.Classes.== "stop" -> CreateChatCompletionResponseChoices'FinishReason'EnumStop
                                             | val GHC.Classes.== "length" -> CreateChatCompletionResponseChoices'FinishReason'EnumLength
                                             | val GHC.Classes.== "tool_calls" -> CreateChatCompletionResponseChoices'FinishReason'EnumToolCalls
                                             | val GHC.Classes.== "content_filter" -> CreateChatCompletionResponseChoices'FinishReason'EnumContentFilter
                                             | val GHC.Classes.== "function_call" -> CreateChatCompletionResponseChoices'FinishReason'EnumFunctionCall
                                             | GHC.Base.otherwise -> CreateChatCompletionResponseChoices'FinishReason'Other val)}
-- | Defines the object schema located at @components.schemas.CreateChatCompletionResponse.properties.choices.items.properties.logprobs@ in the specification.
-- 
-- Log probability information for the choice.
data CreateChatCompletionResponseChoices'Logprobs'NonNullable = CreateChatCompletionResponseChoices'Logprobs'NonNullable {
  -- | content: A list of message content tokens with log probability information.
  createChatCompletionResponseChoices'Logprobs'NonNullableContent :: (OpenAI.Common.Nullable ([ChatCompletionTokenLogprob]))
  } deriving (GHC.Show.Show
  , GHC.Classes.Eq)
instance Data.Aeson.Types.ToJSON.ToJSON CreateChatCompletionResponseChoices'Logprobs'NonNullable
    where {toJSON obj = Data.Aeson.Types.Internal.object (Data.Foldable.concat (["content" Data.Aeson.Types.ToJSON..= createChatCompletionResponseChoices'Logprobs'NonNullableContent obj] : GHC.Base.mempty));
           toEncoding obj = Data.Aeson.Encoding.Internal.pairs (GHC.Base.mconcat (Data.Foldable.concat (["content" Data.Aeson.Types.ToJSON..= createChatCompletionResponseChoices'Logprobs'NonNullableContent obj] : GHC.Base.mempty)))}
instance Data.Aeson.Types.FromJSON.FromJSON CreateChatCompletionResponseChoices'Logprobs'NonNullable
    where {parseJSON = Data.Aeson.Types.FromJSON.withObject "CreateChatCompletionResponseChoices'Logprobs'NonNullable" (\obj -> GHC.Base.pure CreateChatCompletionResponseChoices'Logprobs'NonNullable GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..: "content"))}
-- | Create a new 'CreateChatCompletionResponseChoices'Logprobs'NonNullable' with all required fields.
mkCreateChatCompletionResponseChoices'Logprobs'NonNullable :: OpenAI.Common.Nullable ([ChatCompletionTokenLogprob]) -- ^ 'createChatCompletionResponseChoices'Logprobs'NonNullableContent'
  -> CreateChatCompletionResponseChoices'Logprobs'NonNullable
mkCreateChatCompletionResponseChoices'Logprobs'NonNullable createChatCompletionResponseChoices'Logprobs'NonNullableContent = CreateChatCompletionResponseChoices'Logprobs'NonNullable{createChatCompletionResponseChoices'Logprobs'NonNullableContent = createChatCompletionResponseChoices'Logprobs'NonNullableContent}
