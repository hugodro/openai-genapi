-- CHANGE WITH CAUTION: This is a generated code file generated by https://github.com/Haskell-OpenAPI-Code-Generator/Haskell-OpenAPI-Client-Code-Generator.

{-# LANGUAGE OverloadedStrings #-}
{-# LANGUAGE MultiWayIf #-}

-- | Contains the types generated from the schema CreateCompletionRequest
module OpenAI.Types.CreateCompletionRequest where

import qualified Prelude as GHC.Integer.Type
import qualified Prelude as GHC.Maybe
import qualified Control.Monad.Fail
import qualified Data.Aeson
import qualified Data.Aeson as Data.Aeson.Encoding.Internal
import qualified Data.Aeson as Data.Aeson.Types
import qualified Data.Aeson as Data.Aeson.Types.FromJSON
import qualified Data.Aeson as Data.Aeson.Types.ToJSON
import qualified Data.Aeson as Data.Aeson.Types.Internal
import qualified Data.ByteString
import qualified Data.ByteString as Data.ByteString.Internal
import qualified Data.Foldable
import qualified Data.Functor
import qualified Data.Maybe
import qualified Data.Scientific
import qualified Data.Text
import qualified Data.Text as Data.Text.Internal
import qualified Data.Time.Calendar as Data.Time.Calendar.Days
import qualified Data.Time.LocalTime as Data.Time.LocalTime.Internal.ZonedTime
import qualified GHC.Base
import qualified GHC.Classes
import qualified GHC.Int
import qualified GHC.Show
import qualified GHC.Types
import qualified OpenAI.Common
import OpenAI.TypeAlias

-- | Defines the object schema located at @components.schemas.CreateCompletionRequest@ in the specification.
-- 
-- 
data CreateCompletionRequest = CreateCompletionRequest {
  -- | best_of: Generates \`best_of\` completions server-side and returns the \"best\" (the one with the highest log probability per token). Results cannot be streamed.
  -- 
  -- When used with \`n\`, \`best_of\` controls the number of candidate completions and \`n\` specifies how many to return â€“ \`best_of\` must be greater than \`n\`.
  -- 
  -- **Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for \`max_tokens\` and \`stop\`.
  -- 
  -- 
  -- Constraints:
  -- 
  -- * Maxium  of 20.0
  -- * Minimum  of 0.0
  createCompletionRequestBestOf :: (GHC.Maybe.Maybe (OpenAI.Common.Nullable GHC.Types.Int))
  -- | echo: Echo back the prompt in addition to the completion
  , createCompletionRequestEcho :: (GHC.Maybe.Maybe (OpenAI.Common.Nullable GHC.Types.Bool))
  -- | frequency_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model\'s likelihood to repeat the same line verbatim.
  -- 
  -- [See more information about frequency and presence penalties.](\/docs\/guides\/text-generation\/parameter-details)
  -- 
  -- 
  -- Constraints:
  -- 
  -- * Maxium  of 2.0
  -- * Minimum  of -2.0
  , createCompletionRequestFrequencyPenalty :: (GHC.Maybe.Maybe (OpenAI.Common.Nullable GHC.Types.Double))
  -- | logit_bias: Modify the likelihood of specified tokens appearing in the completion.
  -- 
  -- Accepts a JSON object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this [tokenizer tool](\/tokenizer?view=bpe) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
  -- 
  -- As an example, you can pass \`{\"50256\": -100}\` to prevent the \<|endoftext|> token from being generated.
  , createCompletionRequestLogitBias :: (GHC.Maybe.Maybe (OpenAI.Common.Nullable Data.Aeson.Types.Internal.Object))
  -- | logprobs: Include the log probabilities on the \`logprobs\` most likely output tokens, as well the chosen tokens. For example, if \`logprobs\` is 5, the API will return a list of the 5 most likely tokens. The API will always return the \`logprob\` of the sampled token, so there may be up to \`logprobs+1\` elements in the response.
  -- 
  -- The maximum value for \`logprobs\` is 5.
  -- 
  -- 
  -- Constraints:
  -- 
  -- * Maxium  of 5.0
  -- * Minimum  of 0.0
  , createCompletionRequestLogprobs :: (GHC.Maybe.Maybe (OpenAI.Common.Nullable GHC.Types.Int))
  -- | max_tokens: The maximum number of [tokens](\/tokenizer) that can be generated in the completion.
  -- 
  -- The token count of your prompt plus \`max_tokens\` cannot exceed the model\'s context length. [Example Python code](https:\/\/cookbook.openai.com\/examples\/how_to_count_tokens_with_tiktoken) for counting tokens.
  -- 
  -- 
  -- Constraints:
  -- 
  -- * Minimum  of 0.0
  , createCompletionRequestMaxTokens :: (GHC.Maybe.Maybe (OpenAI.Common.Nullable GHC.Types.Int))
  -- | model: ID of the model to use. You can use the [List models](\/docs\/api-reference\/models\/list) API to see all of your available models, or see our [Model overview](\/docs\/models\/overview) for descriptions of them.
  , createCompletionRequestModel :: CreateCompletionRequestModel'Variants
  -- | n: How many completions to generate for each prompt.
  -- 
  -- **Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for \`max_tokens\` and \`stop\`.
  -- 
  -- 
  -- Constraints:
  -- 
  -- * Maxium  of 128.0
  -- * Minimum  of 1.0
  , createCompletionRequestN :: (GHC.Maybe.Maybe (OpenAI.Common.Nullable GHC.Types.Int))
  -- | presence_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model\'s likelihood to talk about new topics.
  -- 
  -- [See more information about frequency and presence penalties.](\/docs\/guides\/text-generation\/parameter-details)
  -- 
  -- 
  -- Constraints:
  -- 
  -- * Maxium  of 2.0
  -- * Minimum  of -2.0
  , createCompletionRequestPresencePenalty :: (GHC.Maybe.Maybe (OpenAI.Common.Nullable GHC.Types.Double))
  -- | prompt: The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays.
  -- 
  -- Note that \<|endoftext|> is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document.
  , createCompletionRequestPrompt :: (OpenAI.Common.Nullable CreateCompletionRequestPrompt'NonNullableVariants)
  -- | seed: If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same \`seed\` and parameters should return the same result.
  -- 
  -- Determinism is not guaranteed, and you should refer to the \`system_fingerprint\` response parameter to monitor changes in the backend.
  -- 
  -- 
  -- Constraints:
  -- 
  -- * Maxium  of 9.223372e18
  -- * Minimum  of -9.223372e18
  , createCompletionRequestSeed :: (GHC.Maybe.Maybe (OpenAI.Common.Nullable GHC.Types.Int))
  -- | stop: Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.
  , createCompletionRequestStop :: (GHC.Maybe.Maybe (OpenAI.Common.Nullable CreateCompletionRequestStop'NonNullableVariants))
  -- | stream: Whether to stream back partial progress. If set, tokens will be sent as data-only [server-sent events](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/API\/Server-sent_events\/Using_server-sent_events\#Event_stream_format) as they become available, with the stream terminated by a \`data: [DONE]\` message. [Example Python code](https:\/\/cookbook.openai.com\/examples\/how_to_stream_completions).
  , createCompletionRequestStream :: (GHC.Maybe.Maybe (OpenAI.Common.Nullable GHC.Types.Bool))
  -- | suffix: The suffix that comes after a completion of inserted text.
  , createCompletionRequestSuffix :: (GHC.Maybe.Maybe (OpenAI.Common.Nullable Data.Text.Internal.Text))
  -- | temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
  -- 
  -- We generally recommend altering this or \`top_p\` but not both.
  -- 
  -- 
  -- Constraints:
  -- 
  -- * Maxium  of 2.0
  -- * Minimum  of 0.0
  , createCompletionRequestTemperature :: (GHC.Maybe.Maybe (OpenAI.Common.Nullable GHC.Types.Double))
  -- | top_p: An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
  -- 
  -- We generally recommend altering this or \`temperature\` but not both.
  -- 
  -- 
  -- Constraints:
  -- 
  -- * Maxium  of 1.0
  -- * Minimum  of 0.0
  , createCompletionRequestTopP :: (GHC.Maybe.Maybe (OpenAI.Common.Nullable GHC.Types.Double))
  -- | user: A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](\/docs\/guides\/safety-best-practices\/end-user-ids).
  , createCompletionRequestUser :: (GHC.Maybe.Maybe Data.Text.Internal.Text)
  } deriving (GHC.Show.Show
  , GHC.Classes.Eq)
instance Data.Aeson.Types.ToJSON.ToJSON CreateCompletionRequest
    where {toJSON obj = Data.Aeson.Types.Internal.object (Data.Foldable.concat (Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("best_of" Data.Aeson.Types.ToJSON..=)) (createCompletionRequestBestOf obj) : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("echo" Data.Aeson.Types.ToJSON..=)) (createCompletionRequestEcho obj) : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("frequency_penalty" Data.Aeson.Types.ToJSON..=)) (createCompletionRequestFrequencyPenalty obj) : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("logit_bias" Data.Aeson.Types.ToJSON..=)) (createCompletionRequestLogitBias obj) : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("logprobs" Data.Aeson.Types.ToJSON..=)) (createCompletionRequestLogprobs obj) : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("max_tokens" Data.Aeson.Types.ToJSON..=)) (createCompletionRequestMaxTokens obj) : ["model" Data.Aeson.Types.ToJSON..= createCompletionRequestModel obj] : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("n" Data.Aeson.Types.ToJSON..=)) (createCompletionRequestN obj) : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("presence_penalty" Data.Aeson.Types.ToJSON..=)) (createCompletionRequestPresencePenalty obj) : ["prompt" Data.Aeson.Types.ToJSON..= createCompletionRequestPrompt obj] : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("seed" Data.Aeson.Types.ToJSON..=)) (createCompletionRequestSeed obj) : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("stop" Data.Aeson.Types.ToJSON..=)) (createCompletionRequestStop obj) : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("stream" Data.Aeson.Types.ToJSON..=)) (createCompletionRequestStream obj) : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("suffix" Data.Aeson.Types.ToJSON..=)) (createCompletionRequestSuffix obj) : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("temperature" Data.Aeson.Types.ToJSON..=)) (createCompletionRequestTemperature obj) : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("top_p" Data.Aeson.Types.ToJSON..=)) (createCompletionRequestTopP obj) : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("user" Data.Aeson.Types.ToJSON..=)) (createCompletionRequestUser obj) : GHC.Base.mempty));
           toEncoding obj = Data.Aeson.Encoding.Internal.pairs (GHC.Base.mconcat (Data.Foldable.concat (Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("best_of" Data.Aeson.Types.ToJSON..=)) (createCompletionRequestBestOf obj) : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("echo" Data.Aeson.Types.ToJSON..=)) (createCompletionRequestEcho obj) : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("frequency_penalty" Data.Aeson.Types.ToJSON..=)) (createCompletionRequestFrequencyPenalty obj) : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("logit_bias" Data.Aeson.Types.ToJSON..=)) (createCompletionRequestLogitBias obj) : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("logprobs" Data.Aeson.Types.ToJSON..=)) (createCompletionRequestLogprobs obj) : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("max_tokens" Data.Aeson.Types.ToJSON..=)) (createCompletionRequestMaxTokens obj) : ["model" Data.Aeson.Types.ToJSON..= createCompletionRequestModel obj] : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("n" Data.Aeson.Types.ToJSON..=)) (createCompletionRequestN obj) : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("presence_penalty" Data.Aeson.Types.ToJSON..=)) (createCompletionRequestPresencePenalty obj) : ["prompt" Data.Aeson.Types.ToJSON..= createCompletionRequestPrompt obj] : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("seed" Data.Aeson.Types.ToJSON..=)) (createCompletionRequestSeed obj) : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("stop" Data.Aeson.Types.ToJSON..=)) (createCompletionRequestStop obj) : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("stream" Data.Aeson.Types.ToJSON..=)) (createCompletionRequestStream obj) : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("suffix" Data.Aeson.Types.ToJSON..=)) (createCompletionRequestSuffix obj) : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("temperature" Data.Aeson.Types.ToJSON..=)) (createCompletionRequestTemperature obj) : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("top_p" Data.Aeson.Types.ToJSON..=)) (createCompletionRequestTopP obj) : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("user" Data.Aeson.Types.ToJSON..=)) (createCompletionRequestUser obj) : GHC.Base.mempty)))}
instance Data.Aeson.Types.FromJSON.FromJSON CreateCompletionRequest
    where {parseJSON = Data.Aeson.Types.FromJSON.withObject "CreateCompletionRequest" (\obj -> ((((((((((((((((GHC.Base.pure CreateCompletionRequest GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..:! "best_of")) GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..:! "echo")) GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..:! "frequency_penalty")) GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..:! "logit_bias")) GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..:! "logprobs")) GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..:! "max_tokens")) GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..: "model")) GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..:! "n")) GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..:! "presence_penalty")) GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..: "prompt")) GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..:! "seed")) GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..:! "stop")) GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..:! "stream")) GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..:! "suffix")) GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..:! "temperature")) GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..:! "top_p")) GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..:! "user"))}
-- | Create a new 'CreateCompletionRequest' with all required fields.
mkCreateCompletionRequest :: CreateCompletionRequestModel'Variants -- ^ 'createCompletionRequestModel'
  -> OpenAI.Common.Nullable CreateCompletionRequestPrompt'NonNullableVariants -- ^ 'createCompletionRequestPrompt'
  -> CreateCompletionRequest
mkCreateCompletionRequest createCompletionRequestModel createCompletionRequestPrompt = CreateCompletionRequest{createCompletionRequestBestOf = GHC.Maybe.Nothing,
                                                                                                               createCompletionRequestEcho = GHC.Maybe.Nothing,
                                                                                                               createCompletionRequestFrequencyPenalty = GHC.Maybe.Nothing,
                                                                                                               createCompletionRequestLogitBias = GHC.Maybe.Nothing,
                                                                                                               createCompletionRequestLogprobs = GHC.Maybe.Nothing,
                                                                                                               createCompletionRequestMaxTokens = GHC.Maybe.Nothing,
                                                                                                               createCompletionRequestModel = createCompletionRequestModel,
                                                                                                               createCompletionRequestN = GHC.Maybe.Nothing,
                                                                                                               createCompletionRequestPresencePenalty = GHC.Maybe.Nothing,
                                                                                                               createCompletionRequestPrompt = createCompletionRequestPrompt,
                                                                                                               createCompletionRequestSeed = GHC.Maybe.Nothing,
                                                                                                               createCompletionRequestStop = GHC.Maybe.Nothing,
                                                                                                               createCompletionRequestStream = GHC.Maybe.Nothing,
                                                                                                               createCompletionRequestSuffix = GHC.Maybe.Nothing,
                                                                                                               createCompletionRequestTemperature = GHC.Maybe.Nothing,
                                                                                                               createCompletionRequestTopP = GHC.Maybe.Nothing,
                                                                                                               createCompletionRequestUser = GHC.Maybe.Nothing}
-- | Defines the enum schema located at @components.schemas.CreateCompletionRequest.properties.model.anyOf@ in the specification.
-- 
-- 
data CreateCompletionRequestModel'OneOf2 =
   CreateCompletionRequestModel'OneOf2Other Data.Aeson.Types.Internal.Value -- ^ This case is used if the value encountered during decoding does not match any of the provided cases in the specification.
  | CreateCompletionRequestModel'OneOf2Typed Data.Text.Internal.Text -- ^ This constructor can be used to send values to the server which are not present in the specification yet.
  | CreateCompletionRequestModel'OneOf2EnumGpt_3'5TurboInstruct -- ^ Represents the JSON value @"gpt-3.5-turbo-instruct"@
  | CreateCompletionRequestModel'OneOf2EnumDavinci_002 -- ^ Represents the JSON value @"davinci-002"@
  | CreateCompletionRequestModel'OneOf2EnumBabbage_002 -- ^ Represents the JSON value @"babbage-002"@
  deriving (GHC.Show.Show, GHC.Classes.Eq)
instance Data.Aeson.Types.ToJSON.ToJSON CreateCompletionRequestModel'OneOf2
    where {toJSON (CreateCompletionRequestModel'OneOf2Other val) = val;
           toJSON (CreateCompletionRequestModel'OneOf2Typed val) = Data.Aeson.Types.ToJSON.toJSON val;
           toJSON (CreateCompletionRequestModel'OneOf2EnumGpt_3'5TurboInstruct) = "gpt-3.5-turbo-instruct";
           toJSON (CreateCompletionRequestModel'OneOf2EnumDavinci_002) = "davinci-002";
           toJSON (CreateCompletionRequestModel'OneOf2EnumBabbage_002) = "babbage-002"}
instance Data.Aeson.Types.FromJSON.FromJSON CreateCompletionRequestModel'OneOf2
    where {parseJSON val = GHC.Base.pure (if | val GHC.Classes.== "gpt-3.5-turbo-instruct" -> CreateCompletionRequestModel'OneOf2EnumGpt_3'5TurboInstruct
                                             | val GHC.Classes.== "davinci-002" -> CreateCompletionRequestModel'OneOf2EnumDavinci_002
                                             | val GHC.Classes.== "babbage-002" -> CreateCompletionRequestModel'OneOf2EnumBabbage_002
                                             | GHC.Base.otherwise -> CreateCompletionRequestModel'OneOf2Other val)}
-- | Defines the oneOf schema located at @components.schemas.CreateCompletionRequest.properties.model.anyOf@ in the specification.
-- 
-- ID of the model to use. You can use the [List models](\/docs\/api-reference\/models\/list) API to see all of your available models, or see our [Model overview](\/docs\/models\/overview) for descriptions of them.
data CreateCompletionRequestModel'Variants =
   CreateCompletionRequestModel'Text Data.Text.Internal.Text
  | CreateCompletionRequestModel'CreateCompletionRequestModel'OneOf2 CreateCompletionRequestModel'OneOf2
  deriving (GHC.Show.Show, GHC.Classes.Eq)
instance Data.Aeson.Types.ToJSON.ToJSON CreateCompletionRequestModel'Variants
    where {toJSON (CreateCompletionRequestModel'Text a) = Data.Aeson.Types.ToJSON.toJSON a;
           toJSON (CreateCompletionRequestModel'CreateCompletionRequestModel'OneOf2 a) = Data.Aeson.Types.ToJSON.toJSON a}
instance Data.Aeson.Types.FromJSON.FromJSON CreateCompletionRequestModel'Variants
    where {parseJSON val = case (CreateCompletionRequestModel'Text Data.Functor.<$> Data.Aeson.Types.FromJSON.fromJSON val) GHC.Base.<|> ((CreateCompletionRequestModel'CreateCompletionRequestModel'OneOf2 Data.Functor.<$> Data.Aeson.Types.FromJSON.fromJSON val) GHC.Base.<|> Data.Aeson.Types.Internal.Error "No variant matched") of
                           {Data.Aeson.Types.Internal.Success a -> GHC.Base.pure a;
                            Data.Aeson.Types.Internal.Error a -> Control.Monad.Fail.fail a}}
-- | Defines the oneOf schema located at @components.schemas.CreateCompletionRequest.properties.prompt.oneOf@ in the specification.
-- 
-- The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays.
-- 
-- Note that \<|endoftext|> is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document.
data CreateCompletionRequestPrompt'NonNullableVariants =
   CreateCompletionRequestPrompt'NonNullableText Data.Text.Internal.Text
  | CreateCompletionRequestPrompt'NonNullableListTText ([Data.Text.Internal.Text])
  | CreateCompletionRequestPrompt'NonNullableListTInt ([GHC.Types.Int])
  | CreateCompletionRequestPrompt'NonNullableListTListTInt ([[GHC.Types.Int]])
  deriving (GHC.Show.Show, GHC.Classes.Eq)
instance Data.Aeson.Types.ToJSON.ToJSON CreateCompletionRequestPrompt'NonNullableVariants
    where {toJSON (CreateCompletionRequestPrompt'NonNullableText a) = Data.Aeson.Types.ToJSON.toJSON a;
           toJSON (CreateCompletionRequestPrompt'NonNullableListTText a) = Data.Aeson.Types.ToJSON.toJSON a;
           toJSON (CreateCompletionRequestPrompt'NonNullableListTInt a) = Data.Aeson.Types.ToJSON.toJSON a;
           toJSON (CreateCompletionRequestPrompt'NonNullableListTListTInt a) = Data.Aeson.Types.ToJSON.toJSON a}
instance Data.Aeson.Types.FromJSON.FromJSON CreateCompletionRequestPrompt'NonNullableVariants
    where {parseJSON val = case (CreateCompletionRequestPrompt'NonNullableText Data.Functor.<$> Data.Aeson.Types.FromJSON.fromJSON val) GHC.Base.<|> ((CreateCompletionRequestPrompt'NonNullableListTText Data.Functor.<$> Data.Aeson.Types.FromJSON.fromJSON val) GHC.Base.<|> ((CreateCompletionRequestPrompt'NonNullableListTInt Data.Functor.<$> Data.Aeson.Types.FromJSON.fromJSON val) GHC.Base.<|> ((CreateCompletionRequestPrompt'NonNullableListTListTInt Data.Functor.<$> Data.Aeson.Types.FromJSON.fromJSON val) GHC.Base.<|> Data.Aeson.Types.Internal.Error "No variant matched"))) of
                           {Data.Aeson.Types.Internal.Success a -> GHC.Base.pure a;
                            Data.Aeson.Types.Internal.Error a -> Control.Monad.Fail.fail a}}
-- | Defines the oneOf schema located at @components.schemas.CreateCompletionRequest.properties.stop.oneOf@ in the specification.
-- 
-- Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.
data CreateCompletionRequestStop'NonNullableVariants =
   CreateCompletionRequestStop'NonNullableNullableText (OpenAI.Common.Nullable Data.Text.Internal.Text)
  | CreateCompletionRequestStop'NonNullableListTText ([Data.Text.Internal.Text])
  deriving (GHC.Show.Show, GHC.Classes.Eq)
instance Data.Aeson.Types.ToJSON.ToJSON CreateCompletionRequestStop'NonNullableVariants
    where {toJSON (CreateCompletionRequestStop'NonNullableNullableText a) = Data.Aeson.Types.ToJSON.toJSON a;
           toJSON (CreateCompletionRequestStop'NonNullableListTText a) = Data.Aeson.Types.ToJSON.toJSON a}
instance Data.Aeson.Types.FromJSON.FromJSON CreateCompletionRequestStop'NonNullableVariants
    where {parseJSON val = case (CreateCompletionRequestStop'NonNullableNullableText Data.Functor.<$> Data.Aeson.Types.FromJSON.fromJSON val) GHC.Base.<|> ((CreateCompletionRequestStop'NonNullableListTText Data.Functor.<$> Data.Aeson.Types.FromJSON.fromJSON val) GHC.Base.<|> Data.Aeson.Types.Internal.Error "No variant matched") of
                           {Data.Aeson.Types.Internal.Success a -> GHC.Base.pure a;
                            Data.Aeson.Types.Internal.Error a -> Control.Monad.Fail.fail a}}
